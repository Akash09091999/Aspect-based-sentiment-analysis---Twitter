# -*- coding: utf-8 -*-
"""tutorial1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ElZrz_JxUj4jhRjDuPpiS4E-YbWmWBQX

# Capstone Part 2: Data Preparation

For the twitter sentiment analysis model, I have chosen a dataset of tweets labelled either positive or negative.
The dataset for training the model is from "Sentiment140", a dataset originated from Stanford University.
More info on the dataset can be found from the below link.
http://help.sentiment140.com/for-students/<br><br>
The dataset can be downloaded from the below link.<br>
http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip

By looking at the description of the dataset from the link, the information on each field can be found.

0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)<br>
1 - the id of the tweet (2087)<br>
2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)<br>
3 - the query (lyx). If there is no query, then this value is NO_QUERY.<br>
4 - the user that tweeted (robotickilldozr)<br>
5 - the text of the tweet (Lyx is cool)

## First look at the data
"""

import os
pwd = os.getcwd()
print(pwd)

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

"""First, columns names have been assigned to each column."""

cols = ['sentiment','id','date','query_string','user','text']

# !ls "/content/gdrive/My Drive/Colab Notebooks/CNN_series/trainingandtestdata"

df = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/CNN_series/trainingandtestdata/training.1600000.processed.noemoticon.csv",header=None, names=cols)

df.head()

df.info()

df.sentiment.value_counts()

"""Dataset has 1.6million entries, with no null entries, and importantly for the "sentiment" column,
even though the dataset description mentioned neutral class, the training set has no neutral class.
50% of the data is with negative label, and another 50% with positive label.
We can see there's no skewness on the class division.
"""

df.query_string.value_counts()

df.drop(['id','date','query_string','user'],axis=1,inplace=True)

df.head()

"""I first started by dropping the columns that I don't need for the specific purpose of sentiment analysis.<br><br>
"id" column is unique ID for each tweet<br>
"date" column is for date info for the tweet<br>
"query_string" column indicates whether the tweet has been collected with any particular query keyword,
but for this column, 100% of the entries are with value "NO_QUERY"<br>
"user" column is the twitter handle name for the user who tweeted<br><br>
I decided to drop above four columns.
"""

df[df.sentiment == 0].head(10)

df[df.sentiment == 4].head(10)

"""By looking at some entries for each class, it seems like that all the negative class is from 0~799999th index,
and the positive class entries start from 800000 to the end of the dataset.
"""

df[df.sentiment == 0].index

df[df.sentiment == 4].index

"""In order for the computation, I mapped the class value of 4(positive) to 1."""

df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})

df.sentiment.value_counts()

"""# Data Preparation

As a way of sanity check, let's look at the length of the string in text column in each entry.
"""

df['pre_clean_len'] = [len(t) for t in df.text]

"""## Data Dictionary - first draft

Below is the first draft of the data dictionary for the dataset, but as I go through preparation, this will need to be updated.
"""

from pprint import pprint
data_dict = {
    'sentiment':{
        'type':df.sentiment.dtype,
        'description':'sentiment class - 0:negative, 1:positive'
    },
    'text':{
        'type':df.text.dtype,
        'description':'tweet text'
    },
    'pre_clean_len':{
        'type':df.pre_clean_len.dtype,
        'description':'Length of the tweet before cleaning'
    },
    'dataset_shape':df.shape
}

pprint(data_dict)

""" I will also plot pre_clean_len with box plot, so that I can see the overall distribution of length of strings in each entry."""

fig, ax = plt.subplots(figsize=(5, 5))
plt.boxplot(df.pre_clean_len)
plt.show()

"""This looks a bit strange, since the twitter's character limit is 140. But from the above box plot, some of the tweets are way more than 140 chracters long."""

df[df.pre_clean_len > 140].head(10)

"""## Data Preparation 1: HTML decoding

It looks like HTML encoding has not been converted to text, and ended up in text field as '&amp','&quot',etc.<br>
Decoding HTML to general text will be my first step of data preparation.
I will use BeautifulSoup for this.
"""

df.text[279]

from bs4 import BeautifulSoup
example1 = BeautifulSoup(df.text[279], 'lxml')
print(example1.get_text())

"""## Data Preparation 2: @mention

The second part of the preparation is dealing with @mention.<br>
Even though @mention carries a certain information (which another user that the tweet mentioned),
this information doesn't add value to build sentiment analysis model.
"""

df.text[343]

import re
re.sub(r'@[A-Za-z0-9]+','',df.text[343])

"""## Data Preparation 3: URL links

The third part of the cleaning is dealing with URL links, same with @mention,
even though it carries some information, for sentiment analysis purpose,
this can be ignored.
"""

df.text[0]

re.sub('https?://[A-Za-z0-9./]+','',df.text[0])

"""## Data Preparation 4: UTF-8 BOM (Byte Order Mark)"""

df.text[226]

"""By looking at the above entry, I can see strange patterns of characters "\xef\xbf\xbd".
After some researching, I found that these are UTF-8 BOM.<br>
"The UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8."

By decoding text with 'utf-8-sig', this BOM will be replaced with unicode unrecognizable special characters, then I can process this as "?"
"""

testing = df.text[226].decode("utf-8-sig")
testing

testing.replace(u"\ufffd", "?")

"""## Data Preparation 5: hashtag / numbers

Sometimes the text used with a hashtag can provide useful information about the tweet. It might be a bit risky to get rid of all the text together with the hashtag.<br>
So I decided to leave the text intact and just remove the '#'. I will do this in the process of cleaning all the non-letter characters including numbers.
"""

df.text[175]

re.sub("[^a-zA-Z]", " ", df.text[175])

"""# Defining data cleaning function

With above five data cleaning task, I will first define data cleaning function, and then will be applied to the whole dataset. Tokenization, stemming/lemmatization, stop words will be dealt with later stage when creating matrix with either count vectorizer or Tfidf vectorizer.
"""

from nltk.tokenize import WordPunctTokenizer
tok = WordPunctTokenizer()

pat1 = r'@[A-Za-z0-9]+'
pat2 = r'https?://[A-Za-z0-9./]+'
combined_pat = r'|'.join((pat1, pat2))

def tweet_cleaner(text):
    soup = BeautifulSoup(text, 'lxml')
    souped = soup.get_text()
    stripped = re.sub(combined_pat, '', souped)
    try:
        clean = stripped.decode("utf-8-sig").replace(u"\ufffd", "?")
    except:
        clean = stripped
    letters_only = re.sub("[^a-zA-Z]", " ", clean)
    lower_case = letters_only.lower()
    # During the letters_only process two lines above, it has created unnecessay white spaces,
    # I will tokenize and join together to remove unneccessary white spaces
    words = tok.tokenize(lower_case)
    return (" ".join(words)).strip()

testing = df.text[:100]

test_result = []
for t in testing:
    test_result.append(tweet_cleaner(t))

test_result

nums = [0,400000,800000,1200000,1600000]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print "Cleaning and parsing the tweets...\n"
# clean_tweet_texts = []
# for i in xrange(nums[0],nums[1]):
#     if( (i+1)%10000 == 0 ):
#         print "Tweets %d of %d has been processed" % ( i+1, nums[1] )                                                                    
#     clean_tweet_texts.append(tweet_cleaner(df['text'][i]))

len(clean_tweet_texts)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print "Cleaning and parsing the tweets...\n"
# for i in xrange(nums[1],nums[2]):
#     if( (i+1)%10000 == 0 ):
#         print "Tweets %d of %d has been processed" % ( i+1, nums[2] )                                                                    
#     clean_tweet_texts.append(tweet_cleaner(df['text'][i]))

len(clean_tweet_texts)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print "Cleaning and parsing the tweets...\n"
# for i in xrange(nums[2],nums[3]):
#     if( (i+1)%10000 == 0 ):
#         print "Tweets %d of %d has been processed" % ( i+1, nums[3] )                                                                    
#     clean_tweet_texts.append(tweet_cleaner(df['text'][i]))

len(clean_tweet_texts)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print "Cleaning and parsing the tweets...\n"
# for i in xrange(nums[3],nums[4]):
#     if( (i+1)%10000 == 0 ):
#         print "Tweets %d of %d has been processed" % ( i+1, nums[4] )                                                                    
#     clean_tweet_texts.append(tweet_cleaner(df['text'][i]))

len(clean_tweet_texts)

"""## Saving cleaned data as csv"""

clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])
clean_df['target'] = df.sentiment
clean_df.head()

clean_df.to_csv('clean_tweet1.csv',encoding='utf-8')
clean_df.to_csv('clean_tweet2.csv')