# -*- coding: utf-8 -*-
"""tutorial2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nE7AXWHHNhP5UEGevv5ayvZzwqHoMkLK
"""

import os
pwd = os.getcwd()
print(pwd)

from google.colab import drive
drive.mount('/content/gdrive')

"""# Captstone Part 3

Before I move on to EDA, data visualisation. I have made some changes to the data cleaning part.

The first issue I realised is that, during the cleaning process, negation words are split into two parts, and the 't' after the apostrophe vanishes when I filter tokens with length more than one syllable. This makes words like "can't" end up as same as "can". This seems like not a trivial matter for sentiment analysis purpose.

The second issue I realised is that some of the URL links don't start with "http", sometimes people paste a link in "www.aaaa.com" form. This wasn't properly handled when I defined the URL address pattern as 'https?://[A-Za-z0-9./]+'.
And another problem with this regex pattern is that it only detects alphabet, number, period, slash. This means it will fail to catch the part of the URL if it contains any other special character such as "=", "_", "~", etc.</n>

The third issue is the regex pattern for Twitter ID. In the previous cleaning function I defined it as '@[A-Za-z0-9]+', but with a little googling, I found out that twitter ID also allows underscore symbol as a character can be used with ID. Except for underscore symbol, only characters allowed are alphabets and numbers.

Below is the updated datacleaning function.
The order of the cleaning is 
1. Souping
2. BOM removing
3. url address('http:'pattern), twitter ID removing
4. url address('www.'pattern) removing
5. lower-case
6. negation handling
7. removing numbers and special characters
8. tokenizing and joining
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import re
from bs4 import BeautifulSoup
from nltk.tokenize import WordPunctTokenizer
tok = WordPunctTokenizer()

pat1 = r'@[A-Za-z0-9_]+'
pat2 = r'https?://[^ ]+'
combined_pat = r'|'.join((pat1, pat2))
www_pat = r'www.[^ ]+'
negations_dic = {"isn't":"is not", "aren't":"are not", "wasn't":"was not", "weren't":"were not",
                "haven't":"have not","hasn't":"has not","hadn't":"had not","won't":"will not",
                "wouldn't":"would not", "don't":"do not", "doesn't":"does not","didn't":"did not",
                "can't":"can not","couldn't":"could not","shouldn't":"should not","mightn't":"might not",
                "mustn't":"must not"}
neg_pattern = re.compile(r'\b(' + '|'.join(negations_dic.keys()) + r')\b')

def tweet_cleaner_updated(text):
    soup = BeautifulSoup(text, 'lxml')
    souped = soup.get_text()
    try:
        bom_removed = souped.decode("utf-8-sig").replace(u"\ufffd", "?")
    except:
        bom_removed = souped
    stripped = re.sub(combined_pat, '', bom_removed)
    stripped = re.sub(www_pat, '', stripped)
    lower_case = stripped.lower()
    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)
    letters_only = re.sub("[^a-zA-Z]", " ", neg_handled)
    # During the letters_only process two lines above, it has created unnecessay white spaces,
    # I will tokenize and join together to remove unneccessary white spaces
    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]
    return (" ".join(words)).strip()

df = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/CNN_series/trainingandtestdata/training.1600000.processed.noemoticon.csv",header=None,
                 usecols=[0,5],names=['sentiment','text'])
df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print "Cleaning the tweets...\n"
# clean_tweet_texts = []
# for i in xrange(0,len(df)):
#     if( (i+1)%100000 == 0 ):
#         print "Tweets %d of %d has been processed" % ( i+1, len(df) )                                                                    
#     clean_tweet_texts.append(tweet_cleaner_updated(df['text'][i]))

clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])
clean_df['target'] = df.sentiment
clean_df.to_csv('clean_tweet3.csv',encoding='utf-8')

from google.colab import files
files.download("clean_tweet3.csv")

csv = '/content/gdrive/My Drive/Colab Notebooks/CNN_series/clean_tweet1.csv'
my_df = pd.read_csv(csv,index_col=0)
my_df.head()

"""After cleaning the tweets with the updated cleaner function, I took another look at the info()"""

my_df.info()

my_df[my_df.isnull().any(axis=1)].head()

np.sum(my_df.isnull().any(axis=1))

my_df.isnull().any(axis=0)

"""It seems like 3,981 entries have null entries for the text column. This is strange, because the original dataset had no null entries, and if there are any null entries in the cleaned dataset,
it must have happened during the cleaning process.
"""

df = pd.read_csv("/content/gdrive/My Drive/Colab Notebooks/CNN_series/trainingandtestdata/training.1600000.processed.noemoticon.csv",header=None)
df.iloc[my_df[my_df.isnull().any(axis=1)].index,:].head()

"""By looking at these entries in the original data, it seems like only text information they had was either twitter ID or it could have been URL address.
Anyway, these are the info I decided to discard for the sentiment analysis, so I will drop these null rows, and update the data frame.
"""

my_df.dropna(inplace=True)
my_df.reset_index(drop=True,inplace=True)
my_df.info()